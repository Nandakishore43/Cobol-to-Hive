{"name":"Small Steps of Main to BIG","tagline":"Serde for Cobol Layout to Hive table","body":"### Conversion of Mainframe data to Hadoop distributed data:\r\nThis thought poses with two basic questions:\r\n1. How to transfer the data from mainframe to Hadoop?\r\n2. Once transferred, how to interpret the data?\r\n\r\nBefore answering above questions lets examine if there is any advantage in going through this path.  \r\n\r\n### For who is it useful?\r\nCurrently, there are multiple banking and insurance industries who are trying to implement their business models on Hadoop \r\n\r\ntechnologies. For almost all the Hadoop projects there will be a source system file which resides on the Mainframe (hard fact) and that needs to brought over to Hadoop. This process is identified by Sqoop team and tried to create Mainframe file extraction process similar to Database but that is not useful for though. I would redirect the demerits of sqoop mainframe extraction to this Blog but my point here is to highlight the significance of the conversion.\r\n\r\nThis conversion is difficult because we need to establish a translator between different eras of programming models. Once talks EBCIDIC and other ASCII :-)\r\n\r\n### Pain-Areas:\r\nBelow is short-list I could come up with:\r\n<br>1. Data representation conversion from EBCDIC format (Mainframe) to ASCII (JAVA/HADOOP). This is complex because of direct conversion fails because of presence of COMP-3 fileds\r\n<br>2. Field separations are based on offset(Mainframe) instead of separators(HADOOP). \r\n<br>3. Dynamic arrays declartion in mainframe is complex to convert. Dyanic Array length is stored in a field which needs to read first before reading the actual data in array. (For Eg: WS-FIELD OCCURS 1 TO 50 TIMES DEPENDING ON WS-FIELD-LENGTH. )\r\n<br>4. REDEFINES in mainframes is conceptually similar to UNION in C++ but varies a lot is implementation. \r\n<br>5. File Formats in mainframe are Variable length and Fixed Length which needs to handled differently.\r\n<br>6. Folder structure in mainframes is hard to visualize\r\n<br>7. File versions in mainframes (GDG) is a cool feature but hard to handle in HADOOP. \r\n<br>8. Last but not least Computational-Fields conversion.\r\n\r\nWhat!!! Not enough issues to start working on.\r\n\r\n### Enough complaining.. Tell me what Industry is doing now!!\r\nHmm.. :-( Most of the industries are adding additional layers in between mainframe and hadoop to convert the formats. And some are converting mainframes files to desired formats in mainframe systems and then transferring. Others are using Tools like Informatica Power Exchange, SYNCSORT DMX-h etc.\r\n\r\n### My Approach\r\nDevelop a custom Serde which exhibits below properties:<br>\r\n1. Cobol Layout is supplied through TBL PROPERTIES similar to AvroSerde and it will build the hive table definition automatically.<br>\r\n2. Deserailzer should be able to extract the field data based on the offset at runtime.<br>\r\n3. EBCDIC to ASCII to conversion is handled internally<br>\r\n4. ....<br>\r\n\r\n### Benefits:\r\n1. Easier migration from mainframe systems to hadoop<br>\r\n2. Removal of additional layers.<br>\r\n3. Faster processing time.<br>\r\n4. Cost saving because my approach is an open source. YAYYY!!<br>\r\n\r\n### Glimpse of final usage:\r\nMainframe Input File (will be in EBCIDIC format which is unreadable, converted for example purpose)\r\n```\r\nRam Manohar  6123123123123123123king\r\nheheh        5012012012012012comment\r\nLipi         3001001001darling\r\nKanu         2006006loving\r\n```\r\nCobol Layout:\r\n```\r\n01 WS-VAR. \r\n   05 WS-NAME PIC X(12). \r\n   05 WS-MARKS-LENGTH PIC 9(2). \r\n   05 WS-marks OCCURS 0 to 25 TIMES DEPENDING ON WS-MARKS-LENGTH. \r\n      10 WS-MARK PIC 999. \r\n   05 WS-NICKNAME PIC X(6)\r\n```\r\nHive DDL:\r\n```\r\nCREATE TABLE Cobol2Hive\r\nROW FORMAT SERDE 'com.savy3.cobolserde.CobolSerde' \r\nLOCATION '/home/hduser/hive/warehouse/ram.db/lolol'\r\nTBLPROPERTIES ('cobol.layout'='01 WS-VAR. 05 WS-NAME PIC X(12). 05 WS-MARKS-LENGTH PIC 9(2). 05 WS-marks OCCURS 0 to 6 TIMES DEPENDING \r\n\r\nON WS-MARKS-LENGTH. 10 WS-MARK PIC 999. 05 WS-NICKNAME PIC X(6)');\r\n```\r\nOutput:\r\n```\r\nselect * from Cobol2Hive;\r\nOK\r\nws_name       ws_marks_length ws_mark ws_mark_1 ws_mark_2 ws_mark_3 ws_mark_4 ws_mark_5 ws_nickname \r\nRam Manohar \t6               123     123       123       123       123       123   \t  king\r\nheheh       \t5               12      12        12        12        12        null      comment\r\nLipi        \t3               1       1         1         null      null      null      darling\r\nKanu        \t2               6       6         null      null      null      null      loving\r\n```\r\n\r\n### Comments\r\nWant to leave a comment? Visit <a href=\"https://github.com/izuzak/izuzak.github.com/issues/{{page.commentIssueId}}\"> this post's issue \r\n\r\npage on GitHub</a> (you'll need a GitHub account. What? Like you already don't have one?!).\r\n\r\n### Authors and Contributors\r\nAuthor: <a href=\"https://github.com/rbheemana\" class=\"user-mention\">@rbheemana</a> </p>\r\n      <div class=\"owner\">\r\n      <p><a href=\"https://github.com/rbheemana\" class=\"avatar\"><img src=\"https://avatars1.githubusercontent.com/u/12659735?\r\n\r\nv=3&amp;s=60\" width=\"48\" height=\"48\"></a> <a href=\"https://github.com/rbheemana\">rbheemana</a> maintains <a \r\n\r\nhref=\"https://github.com/rbheemana/Cobol-to-Hive\">Cobol-to-hive</a>","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}