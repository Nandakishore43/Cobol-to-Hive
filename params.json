{"name":"Small Steps of Main to BIG","tagline":"Serde for Cobol Layout to Hive table","body":"### Transfer of mainframe file to HDFS \r\n\r\nGenerally, when we need a file from other systems to hadoop, we get the data to edge using FTP/SFTP/CURL etc protocols.\r\nBut when you are FTPing the data from mainframe system (Ofcourse mainframe ftp port should be opened) you need to extra careful to download the data the binary format. This is required since the mainframe uses EBCDIC encoding. One could argue to convert the file to ASCII while FTPing the data. That would work perfectly fine if the file that you are downloading has no usage of any computational fields. Examples of such are copybooks, code, plain text, etc. Problem arises when the data consists of computational fields, one should convert data to ASCII excluding these fields. This action become complicated as the position of the such fields can only be determined by its layout.\r\n\r\nFor now lets import the mainframe data without converting to ASCII, that is in EBCIDIC format. We can achieve this by copying the data to edgenode whatever protocol is common between two systems and then using hdfs dfs -put command to transfer to HDFS.\r\n\r\nPersonally, I have written simple java program to connect to mainframe system using FTP Client API and bring the data directly to the HDFS.\r\n\r\n### Usage of my program to FTP the file\r\n'''''\r\nexport \r\nexport\r\nexport\r\nexport\r\njar CobolSerde.jar <Source: Mainframe File Name> <Target: HDFS File Location>\r\n'''''\r\n\r\n### Code Logic\r\n\r\n\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}